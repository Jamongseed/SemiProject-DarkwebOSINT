# crawler_ransomware_live.py
from bs4 import BeautifulSoup
from pprint import pprint
import requests
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
import re
from pathlib import Path
import csv

# --- í†µí•© ìŠ¤í‚¤ë§ˆ í—¤ë” ---
UNIFIED_HEADERS = [
    "source", "record_type", "id", "company", "website", "country", "address",
    "size_bytes", "size_gib", "is_published", "time_until_publication",
    "posted_at_utc", "crawled_at_utc", "crawled_at_kst",
    "ransomware_group", "discovery_date", "estimated_attack_date",
    "details_url", "description", "files_api_present"
]

def get_html_sync(url: str, timeout: int = 10) -> str | None:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        print(f"âœ… [{url}] - HTML ì½˜í…ì¸  ë¡œë“œ ì„±ê³µ")
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"âŒ [{url}] - ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def parse_ransomware_live_data(html_content):
    kst_timezone = ZoneInfo("Asia/Seoul")
    crawl_time_utc = datetime.now(timezone.utc).isoformat()
    crawl_time_kst = datetime.now(kst_timezone).isoformat()

    soup = BeautifulSoup(html_content, 'html.parser')

    def get_victim_details(item):
        try:
            name = item.select_one('strong').get_text(strip=True)

            group_el = item.select_one('small a span.badge')
            group = group_el.get_text(strip=True) if group_el else ""

            date_container = item.select_one('div.text-body-secondary')
            date_text = date_container.get_text(" ", strip=True) if date_container else ""
            discovery_date_match = re.search(r"Discovery Date: ([\d-]+)", date_text)
            discovery_date = discovery_date_match.group(1) if discovery_date_match else ""
            attack_date_match = re.search(r"Estimated Attack Date: ([\d-]+)", date_text)
            estimated_attack_date = attack_date_match.group(1) if attack_date_match else ""

            description_tag = item.select_one('div.bg-body-secondary')
            description = description_tag.get_text(strip=True) if description_tag else ""

            country_tag = item.select_one('img[style*="width: 32px"]')
            country = country_tag['alt'].strip() if country_tag and country_tag.has_attr('alt') else ""
            country = country.upper() if len(country) == 2 else ""  # ISO-2ë§Œ ìœ ì§€

            website_tag = item.select_one('a:has(i.fa-globe-americas)')
            website = website_tag['href'].strip() if website_tag and website_tag.has_attr('href') else ""

            details_link_tag = item.select_one('a[href*="/id/"]')
            details_url = ("https://www.ransomware.live" + details_link_tag['href'].strip()) if (details_link_tag and details_link_tag.has_attr('href')) else ""
            
            return {
                "company_name": name,
                "ransomware_group": group,
                "discovery_date": discovery_date,
                "estimated_attack_date": estimated_attack_date,
                "description": description,
                "country": country,
                "website": website,
                "details_url": details_url
            }
        except Exception:
            return None

    # (ì„ íƒ) ìƒë‹¨ ì¹´ìš´í„° íŒŒì‹±ì€ ê¸°ì¡´ëŒ€ë¡œ ìœ ì§€
    statistics = {}
    try:
        scripts = soup.find_all('script')
        script_text = ""
        for script in scripts:
            if 'animateCounter' in script.text:
                script_text = script.text
                break

        groups_match = re.search(r"animateCounter\('groupsCounter',\s*\d+,\s*([\d,]+)", script_text)
        victims_match = re.search(r"animateCounter\('victimsCounter',\s*\d+,\s*([\d,]+)", script_text)
        year_match = re.search(r"animateCounter\('victimsThisYearCounter',\s*\d+,\s*([\d,]+)", script_text)
        month_match = re.search(r"animateCounter\('victimsThisMonthCounter',\s*\d+,\s*([\d,]+)", script_text)

        statistics = {
            "Total Groups": int(groups_match.group(1).replace(',', '')) if groups_match else 0,
            "Total Victims": int(victims_match.group(1).replace(',', '')) if victims_match else 0,
            "Victims This Year": int(year_match.group(1).replace(',', '')) if year_match else 0,
            "Victims This Month": int(month_match.group(1).replace(',', '')) if month_match else 0
        }
    except Exception as e:
        print(f"í†µê³„ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        statistics = {}

    victim_items = soup.select('#victim-list .victim-item')
    victims_list = [data for item in victim_items if (data := get_victim_details(item)) is not None]

    return {
        "crawled_at_utc": crawl_time_utc,
        "crawled_at_kst": crawl_time_kst,
        "statistics": statistics,
        "victims": victims_list
    }

def save_csvs(results: dict, out_dir: str = "outputs", prefix: str = "ransomware_live"):
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    # í†µê³„ëŠ” append
    stats_file = out_path / f"{prefix}_stats.csv"
    stats_headers = [
        "crawled_at_utc", "crawled_at_kst",
        "Total Groups", "Total Victims", "Victims This Year", "Victims This Month"
    ]
    write_header = not stats_file.exists()
    with stats_file.open("a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=stats_headers)
        if write_header:
            writer.writeheader()
        writer.writerow({
            "crawled_at_utc": results.get("crawled_at_utc"),
            "crawled_at_kst": results.get("crawled_at_kst"),
            "Total Groups": results.get("statistics", {}).get("Total Groups", 0),
            "Total Victims": results.get("statistics", {}).get("Total Victims", 0),
            "Victims This Year": results.get("statistics", {}).get("Victims This Year", 0),
            "Victims This Month": results.get("statistics", {}).get("Victims This Month", 0),
        })

    # ì›ë³¸ victims.csv (ë®ì–´ì“°ê¸°) - ê¸°ì¡´ í¬ë§· ìœ ì§€ìš©
    victims_file = out_path / f"{prefix}_victims.csv"
    victim_headers = [
        "crawled_at_utc", "crawled_at_kst",
        "company_name", "ransomware_group",
        "discovery_date", "estimated_attack_date",
        "description", "country", "website", "details_url"
    ]
    with victims_file.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=victim_headers)
        writer.writeheader()
        for v in results.get("victims", []):
            writer.writerow({
                "crawled_at_utc": results.get("crawled_at_utc"),
                "crawled_at_kst": results.get("crawled_at_kst"),
                **v
            })
    print(f" - ì›ë³¸ í”¼í•´ì: {victims_file.resolve()}")

def save_unified_csv_ransomware(results: dict, out_dir: str = "outputs",
                                filename: str = "ransomware_live_unified.csv"):
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)
    path = out / filename

    with path.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=UNIFIED_HEADERS)
        w.writeheader()

        crawled_at_utc = results.get("crawled_at_utc", "")
        crawled_at_kst = results.get("crawled_at_kst", "")
        for v in results.get("victims", []):
            rid = v.get("details_url") or f'{v.get("company_name","")}|{v.get("ransomware_group","")}|{v.get("discovery_date","")}'
            # êµ­ê°€/ì›¹ì‚¬ì´íŠ¸ ë“± ê²°ì¸¡ì€ ì´ë¯¸ parse ë‹¨ê³„ì—ì„œ ë¹ˆì¹¸ ì²˜ë¦¬ë¨
            w.writerow({
                "source": "ransomware.live",
                "record_type": "victim",
                "id": rid,
                "company": v.get("company_name", ""),
                "website": v.get("website", ""),
                "country": v.get("country", ""),
                "address": "",
                "size_bytes": "",
                "size_gib": "",
                "is_published": "",
                "time_until_publication": "",
                "posted_at_utc": "",
                "crawled_at_utc": crawled_at_utc,
                "crawled_at_kst": crawled_at_kst,
                "ransomware_group": v.get("ransomware_group", ""),
                "discovery_date": v.get("discovery_date", ""),
                "estimated_attack_date": v.get("estimated_attack_date", ""),
                "details_url": v.get("details_url", ""),
                "description": v.get("description", ""),
                "files_api_present": ""
            })
    print(f" - í†µí•©(ë®ì–´ì“°ê¸°): {path.resolve()}")

def main():
    URL = "https://www.ransomware.live/"
    print(f"'{URL}'ì—ì„œ ë°ì´í„° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    html_content = get_html_sync(URL)

    if html_content:
        print("\ní¬ë¡¤ë§ ì„±ê³µ! ë°ì´í„° íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        ransomware_data = parse_ransomware_live_data(html_content)

        print("\n--- íŒŒì‹± ì™„ë£Œëœ ë°ì´í„° ---")
        pprint(ransomware_data)

        save_csvs(ransomware_data, out_dir="outputs", prefix="ransomware_live")
        save_unified_csv_ransomware(ransomware_data, out_dir="outputs",
                                    filename="ransomware_live_unified.csv")
        print("\nğŸ‰ í”„ë¡œê·¸ë¨ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâ—ï¸ HTML ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ íŒŒì‹±ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("í”„ë¡œê·¸ë¨ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

